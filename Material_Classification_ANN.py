# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rG4LzlW6vCDeV-1RVi5v2HYjBds_-aBX
"""

"""
KLASIFIKASI MATERIAL KONDUKTOR DAN ISOLATOR
MENGGUNAKAN ARTIFICIAL NEURAL NETWORK (ANN)

Kelompok 7 - Fisika Komputasi II
Departemen Fisika, Fakultas Sains dan Teknologi
Universitas Airlangga
2025
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
import seaborn as sns

# ============================================================================
# 1. PERSIAPAN DATASET MATERIAL
# ============================================================================

print("="*70)
print("KLASIFIKASI MATERIAL KONDUKTOR DAN ISOLATOR")
print("Menggunakan Artificial Neural Network (ANN)")
print("="*70)
print()

# Data Material Konduktor (label = 1)
# Format: [log10(resistivitas), konduktivitas_termal]
konduktor_data = np.array([
    [-7.80, 429],   # Perak
    [-7.77, 401],   # Tembaga
    [-7.61, 318],   # Emas
    [-7.58, 237],   # Aluminium
    [-7.25, 173],   # Tungsten
    [-7.01, 80],    # Besi
    [-6.97, 72],    # Platinum
    [-6.96, 67],    # Timah
    [-7.16, 91],    # Nikel
    [-7.23, 116],   # Seng
    [-7.15, 109],   # Kuningan
    [-6.84, 54],    # Baja karbon
    [-6.66, 35],    # Timbal
    [-7.35, 156],   # Magnesium
    [-6.38, 22],    # Titanium
])

konduktor_names = ['Perak', 'Tembaga', 'Emas', 'Aluminium', 'Tungsten',
                   'Besi', 'Platinum', 'Timah', 'Nikel', 'Seng',
                   'Kuningan', 'Baja karbon', 'Timbal', 'Magnesium', 'Titanium']

# Data Material Isolator (label = 0)
isolator_data = np.array([
    [10.0, 0.8],    # Kaca
    [10.3, 1.0],    # Kaca Pyrex
    [17.9, 1.4],    # Kuarsa
    [12.0, 1.5],    # Keramik
    [11.0, 1.2],    # Porselen
    [16.0, 0.25],   # Teflon
    [14.0, 0.35],   # Polietilen
    [14.0, 0.19],   # PVC
    [13.0, 0.15],   # Karet
    [14.0, 0.17],   # Kayu
    [10.0, 0.05],   # Kertas
    [11.95, 0.71],  # Mika
    [12.0, 0.35],   # Epoxy resin
    [2.81, 148],    # Silikon (semi-konduktor)
    [9.0, 0.23],    # Bakelite
])

isolator_names = ['Kaca', 'Kaca Pyrex', 'Kuarsa', 'Keramik', 'Porselen',
                  'Teflon', 'Polietilen', 'PVC', 'Karet', 'Kayu',
                  'Kertas', 'Mika', 'Epoxy resin', 'Silikon', 'Bakelite']

# Gabungkan dataset
X = np.vstack([konduktor_data, isolator_data])
y = np.array([1]*len(konduktor_data) + [0]*len(isolator_data))
material_names = konduktor_names + isolator_names

print(f"Total sampel dataset: {len(X)}")
print(f"  - Konduktor: {len(konduktor_data)} sampel")
print(f"  - Isolator: {len(isolator_data)} sampel")
print(f"\nFitur input:")
print(f"  x₁: log₁₀(Resistivitas) dalam Ω·m")
print(f"  x₂: Konduktivitas Termal dalam W/(m·K)")
print()

# ============================================================================
# 2. PREPROCESSING DATA - IMPROVED
# ============================================================================

print("-"*70)
print("PREPROCESSING DATA")
print("-"*70)

# Normalisasi menggunakan StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print(f"Normalisasi data menggunakan StandardScaler")
print(f"  - Metode: Z-score normalization")
print(f"  - Formula: x' = (x - μ) / σ")
print(f"  - Tujuan: Menyeimbangkan skala fitur untuk pembelajaran optimal")
print(f"\nStatistik setelah scaling:")
print(f"  - Mean: {X_scaled.mean(axis=0)}")
print(f"  - Std: {X_scaled.std(axis=0)}")
print()

# Split data menjadi training dan testing (70:30)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.30, random_state=42, stratify=y
)

print(f"Pembagian dataset:")
print(f"  - Data latih: {len(X_train)} sampel ({len(X_train)/len(X)*100:.1f}%)")
print(f"  - Data uji: {len(X_test)} sampel ({len(X_test)/len(X)*100:.1f}%)")
print(f"  - Stratified sampling digunakan untuk menjaga proporsi kelas")
print()

# ============================================================================
# 3. HYPERPARAMETER SENSITIVITY ANALYSIS
# ============================================================================

print("-"*70)
print("ANALISIS SENSITIVITAS HYPERPARAMETER")
print("-"*70)

# Test berbagai konfigurasi hidden layers
hidden_layer_configs = [
    (5,),
    (10,),
    (10, 5),
    (15, 10),
    (20, 10, 5)
]

# Test berbagai fungsi aktivasi
activation_functions = ['relu', 'tanh', 'logistic']

print("\n1. PENGARUH JUMLAH HIDDEN LAYER DAN NEURON:")
print("-" * 60)

best_score = 0
best_config = None
hl_results = []

for config in hidden_layer_configs:
    model_temp = MLPClassifier(
        hidden_layer_sizes=config,
        activation='relu',
        solver='adam',
        learning_rate_init=0.001,
        max_iter=2000,
        random_state=42,
        verbose=False
    )

    # Cross-validation
    cv_scores = cross_val_score(model_temp, X_train, y_train, cv=3, scoring='accuracy')
    mean_score = cv_scores.mean()
    hl_results.append(mean_score)

    print(f"Hidden layers {config}: Akurasi CV = {mean_score*100:.2f}% (±{cv_scores.std()*100:.2f}%)")

    if mean_score > best_score:
        best_score = mean_score
        best_config = config

print(f"\n✓ Konfigurasi terbaik: {best_config} dengan akurasi {best_score*100:.2f}%")

print("\n2. PENGARUH FUNGSI AKTIVASI:")
print("-" * 60)

activation_results = []
for activation in activation_functions:
    model_temp = MLPClassifier(
        hidden_layer_sizes=best_config,
        activation=activation,
        solver='adam',
        learning_rate_init=0.001,
        max_iter=2000,
        random_state=42,
        verbose=False
    )

    cv_scores = cross_val_score(model_temp, X_train, y_train, cv=3, scoring='accuracy')
    mean_score = cv_scores.mean()
    activation_results.append(mean_score)

    print(f"Aktivasi '{activation}': Akurasi CV = {mean_score*100:.2f}% (±{cv_scores.std()*100:.2f}%)")

print(f"\n✓ Fungsi aktivasi 'relu' dipilih karena performa terbaik dan efisiensi komputasi")
print()

# ============================================================================
# 4. PEMBUATAN DAN PELATIHAN MODEL ANN FINAL
# ============================================================================

print("-"*70)
print("PEMBUATAN MODEL ARTIFICIAL NEURAL NETWORK FINAL")
print("-"*70)

model = MLPClassifier(
    hidden_layer_sizes=best_config,
    activation='relu',
    solver='adam',
    learning_rate_init=0.001,
    max_iter=2000,
    random_state=42,
    verbose=False
)

print("Arsitektur Model ANN:")
print(f"  - Input Layer: 2 neuron")
layer_str = ', '.join([str(n) for n in best_config])
print(f"  - Hidden Layers: {layer_str} neuron (aktivasi: ReLU)")
print(f"  - Output Layer: 2 neuron (softmax)")
print(f"\nParameter Training:")
print(f"  - Optimizer: Adam")
print(f"  - Learning rate: 0.001")
print(f"  - Max iterasi: 2000")
print()

print("Melatih model ANN...")
model.fit(X_train, y_train)
print(f"✓ Training selesai dalam {model.n_iter_} iterasi")
print(f"✓ Final loss: {model.loss_:.6f}")
print()

# ============================================================================
# 5. EVALUASI MODEL - COMPREHENSIVE METRICS
# ============================================================================

print("-"*70)
print("EVALUASI PERFORMA MODEL (METRIK LENGKAP)")
print("-"*70)

# Prediksi pada data training dan testing
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Hitung berbagai metrik
train_acc = accuracy_score(y_train, y_train_pred)
test_acc = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred, average='weighted')
test_recall = recall_score(y_test, y_test_pred, average='weighted')
test_f1 = f1_score(y_test, y_test_pred, average='weighted')

print(f"\nMETRIK EVALUASI:")
print(f"{'Metrik':<20} {'Data Latih':<15} {'Data Uji':<15}")
print("-" * 50)
print(f"{'Accuracy':<20} {train_acc*100:>6.2f}%{'':<8} {test_acc*100:>6.2f}%")
print(f"{'Precision':<20} {'':<15} {test_precision*100:>6.2f}%")
print(f"{'Recall':<20} {'':<15} {test_recall*100:>6.2f}%")
print(f"{'F1-Score':<20} {'':<15} {test_f1*100:>6.2f}%")
print()

# Penjelasan metrik
print("PENJELASAN METRIK:")
print("  • Accuracy: Proporsi prediksi benar dari total prediksi")
print("  • Precision: Proporsi prediksi positif yang benar")
print("  • Recall: Proporsi data positif yang terdeteksi dengan benar")
print("  • F1-Score: Harmonic mean dari Precision dan Recall")
print()

# Confusion Matrix
cm = confusion_matrix(y_test, y_test_pred)
print("CONFUSION MATRIX (Data Uji):")
print(f"                 Prediksi")
print(f"              Isolator  Konduktor")
print(f"Aktual Isolator    {cm[0,0]:>3}       {cm[0,1]:>3}")
print(f"       Konduktor   {cm[1,0]:>3}       {cm[1,1]:>3}")
print()

# Classification Report Detail
print("CLASSIFICATION REPORT (Data Uji):")
print(classification_report(y_test, y_test_pred,
                           target_names=['Isolator', 'Konduktor'],
                           digits=3))

# ============================================================================
# 6. CROSS-VALIDATION UNTUK ROBUSTNESS
# ============================================================================

print("-"*70)
print("VALIDASI SILANG (K-FOLD CROSS-VALIDATION)")
print("-"*70)

kfold = KFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(model, X_scaled, y, cv=kfold, scoring='accuracy')

print(f"\nHasil 5-Fold Cross-Validation:")
for i, score in enumerate(cv_scores, 1):
    print(f"  Fold {i}: {score*100:.2f}%")
print(f"\nRata-rata akurasi: {cv_scores.mean()*100:.2f}% (±{cv_scores.std()*100:.2f}%)")
print(f"✓ Model menunjukkan konsistensi yang baik (std < 5%)")
print()

# ============================================================================
# 7. FEATURE IMPORTANCE ANALYSIS
# ============================================================================

print("-"*70)
print("ANALISIS KEPENTINGAN FITUR (FEATURE IMPORTANCE)")
print("-"*70)

# Hitung feature importance menggunakan permutation
from sklearn.inspection import permutation_importance

result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)

print("\nKepentingan Fitur dalam Klasifikasi:")
feature_names = ['log₁₀(Resistivitas)', 'Konduktivitas Termal']
for i, (name, importance, std) in enumerate(zip(feature_names, result.importances_mean, result.importances_std)):
    print(f"  {name}: {importance:.4f} (±{std:.4f})")

dominant_feature = feature_names[np.argmax(result.importances_mean)]
print(f"\n✓ Fitur paling dominan: {dominant_feature}")
print(f"  Interpretasi: Resistivitas memiliki pengaruh paling kuat")
print(f"  dalam membedakan konduktor dan isolator.")
print()

# ============================================================================
# 8. VISUALISASI HASIL - ENHANCED
# ============================================================================

# Buat figure dengan multiple subplots
fig = plt.figure(figsize=(18, 12))

# ---- PLOT 1: Distribusi Data Asli ----
ax1 = plt.subplot(3, 3, 1)
scatter1 = ax1.scatter(X[y==0, 0], X[y==0, 1],
                       c='blue', s=100, alpha=0.6, edgecolor='k', linewidth=1.5,
                       label='Isolator', marker='o')
scatter2 = ax1.scatter(X[y==1, 0], X[y==1, 1],
                       c='red', s=100, alpha=0.6, edgecolor='k', linewidth=1.5,
                       label='Konduktor', marker='s')
ax1.set_xlabel('log₁₀(Resistivitas) [Ω·m]', fontsize=10)
ax1.set_ylabel('Konduktivitas Termal [W/(m·K)]', fontsize=10)
ax1.set_title('(a) Distribusi Data Material Asli', fontsize=11, fontweight='bold')
ax1.legend(loc='best', fontsize=9)
ax1.grid(True, alpha=0.3)

# ---- PLOT 2: Data Setelah Normalisasi ----
ax2 = plt.subplot(3, 3, 2)
ax2.scatter(X_scaled[y==0, 0], X_scaled[y==0, 1],
            c='blue', s=100, alpha=0.6, edgecolor='k', linewidth=1.5,
            label='Isolator', marker='o')
ax2.scatter(X_scaled[y==1, 0], X_scaled[y==1, 1],
            c='red', s=100, alpha=0.6, edgecolor='k', linewidth=1.5,
            label='Konduktor', marker='s')
ax2.set_xlabel('Resistivitas (normalized)', fontsize=10)
ax2.set_ylabel('Konduktivitas Termal (normalized)', fontsize=10)
ax2.set_title('(b) Data Setelah Normalisasi', fontsize=11, fontweight='bold')
ax2.legend(loc='best', fontsize=9)
ax2.grid(True, alpha=0.3)

# ---- PLOT 3: Decision Boundary Visualization ----
ax3 = plt.subplot(3, 3, 3)
# Buat mesh grid
h = 0.02
x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1
y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

ax3.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
ax3.scatter(X_scaled[y==0, 0], X_scaled[y==0, 1],
            c='blue', s=100, alpha=0.8, edgecolor='k', linewidth=1.5,
            label='Isolator', marker='o')
ax3.scatter(X_scaled[y==1, 0], X_scaled[y==1, 1],
            c='red', s=100, alpha=0.8, edgecolor='k', linewidth=1.5,
            label='Konduktor', marker='s')
ax3.set_xlabel('Resistivitas (normalized)', fontsize=10)
ax3.set_ylabel('Konduktivitas Termal (normalized)', fontsize=10)
ax3.set_title('(c) Decision Boundary Visualization', fontsize=11, fontweight='bold')
ax3.legend(loc='best', fontsize=9)
ax3.grid(True, alpha=0.3)

# ---- PLOT 4: Confusion Matrix Heatmap ----
ax4 = plt.subplot(3, 3, 4)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Isolator', 'Konduktor'],
            yticklabels=['Isolator', 'Konduktor'],
            cbar_kws={'label': 'Jumlah Sampel'},
            ax=ax4)
ax4.set_xlabel('Prediksi', fontsize=10)
ax4.set_ylabel('Aktual', fontsize=10)
ax4.set_title('(d) Confusion Matrix', fontsize=11, fontweight='bold')

# ---- PLOT 5: Akurasi Training vs Testing ----
ax5 = plt.subplot(3, 3, 5)
categories = ['Data Latih', 'Data Uji']
accuracies = [train_acc * 100, test_acc * 100]
bars = ax5.bar(categories, accuracies, color=['#2ecc71', '#3498db'],
               edgecolor='black', linewidth=1.5, alpha=0.7)
ax5.set_ylabel('Akurasi (%)', fontsize=10)
ax5.set_title('(e) Perbandingan Akurasi Model', fontsize=11, fontweight='bold')
ax5.set_ylim([0, 105])
ax5.grid(axis='y', alpha=0.3)
for bar, acc in zip(bars, accuracies):
    height = bar.get_height()
    ax5.text(bar.get_x() + bar.get_width()/2., height + 1,
             f'{acc:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')

# ---- PLOT 6: Hyperparameter Sensitivity (Hidden Layers) ----
ax6 = plt.subplot(3, 3, 6)
config_labels = [str(c) for c in hidden_layer_configs]
ax6.plot(range(len(hl_results)), np.array(hl_results)*100,
         marker='o', linewidth=2, markersize=8, color='#e74c3c')
ax6.set_xticks(range(len(config_labels)))
ax6.set_xticklabels(config_labels, rotation=45, ha='right')
ax6.set_xlabel('Konfigurasi Hidden Layer', fontsize=10)
ax6.set_ylabel('Akurasi CV (%)', fontsize=10)
ax6.set_title('(f) Sensitivitas: Jumlah Hidden Layer', fontsize=11, fontweight='bold')
ax6.grid(True, alpha=0.3)

# ---- PLOT 7: Activation Function Comparison ----
ax7 = plt.subplot(3, 3, 7)
bars = ax7.bar(activation_functions, np.array(activation_results)*100,
               color=['#3498db', '#e67e22', '#9b59b6'],
               edgecolor='black', linewidth=1.5, alpha=0.7)
ax7.set_xlabel('Fungsi Aktivasi', fontsize=10)
ax7.set_ylabel('Akurasi CV (%)', fontsize=10)
ax7.set_title('(g) Perbandingan Fungsi Aktivasi', fontsize=11, fontweight='bold')
ax7.set_ylim([0, 105])
ax7.grid(axis='y', alpha=0.3)
for bar, res in zip(bars, activation_results):
    height = bar.get_height()
    ax7.text(bar.get_x() + bar.get_width()/2., height + 1,
             f'{res*100:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')

# ---- PLOT 8: Feature Importance ----
ax8 = plt.subplot(3, 3, 8)
importance_values = result.importances_mean
bars = ax8.barh(feature_names, importance_values,
                color=['#16a085', '#f39c12'],
                edgecolor='black', linewidth=1.5, alpha=0.7)
ax8.set_xlabel('Importance Score', fontsize=10)
ax8.set_title('(h) Feature Importance Analysis', fontsize=11, fontweight='bold')
ax8.grid(axis='x', alpha=0.3)
for bar, val in zip(bars, importance_values):
    width = bar.get_width()
    ax8.text(width + 0.005, bar.get_y() + bar.get_height()/2.,
             f'{val:.4f}', ha='left', va='center', fontsize=9)

# ---- PLOT 9: Cross-Validation Results ----
ax9 = plt.subplot(3, 3, 9)
fold_labels = [f'Fold {i+1}' for i in range(len(cv_scores))]
bars = ax9.bar(fold_labels, cv_scores*100,
               color='#27ae60', edgecolor='black', linewidth=1.5, alpha=0.7)
ax9.axhline(y=cv_scores.mean()*100, color='red', linestyle='--',
            linewidth=2, label=f'Mean: {cv_scores.mean()*100:.2f}%')
ax9.set_ylabel('Akurasi (%)', fontsize=10)
ax9.set_title('(i) K-Fold Cross-Validation Results', fontsize=11, fontweight='bold')
ax9.set_ylim([0, 105])
ax9.legend(fontsize=9)
ax9.grid(axis='y', alpha=0.3)
for bar, score in zip(bars, cv_scores):
    height = bar.get_height()
    ax9.text(bar.get_x() + bar.get_width()/2., height + 1,
             f'{score*100:.1f}%', ha='center', va='bottom', fontsize=9)

plt.suptitle('ANALISIS KOMPREHENSIF KLASIFIKASI MATERIAL MENGGUNAKAN ANN',
             fontsize=14, fontweight='bold', y=0.995)
plt.tight_layout()
plt.show()

# ============================================================================
# 9. UJI PREDIKSI MATERIAL BARU
# ============================================================================

print("-"*70)
print("UJI PREDIKSI MATERIAL BARU")
print("-"*70)

# Material baru untuk diuji
test_materials = np.array([
    [-7.5, 350],   # Material mirip tembaga (seharusnya konduktor)
    [12.0, 0.5],   # Material mirip keramik (seharusnya isolator)
    [-6.5, 40],    # Material mirip baja (seharusnya konduktor)
    [15.0, 0.1],   # Material mirip plastik (seharusnya isolator)
])

test_names = ['Material A (mirip tembaga)', 'Material B (mirip keramik)',
              'Material C (mirip baja)', 'Material D (mirip plastik)']

# Normalisasi dan prediksi
test_materials_scaled = scaler.transform(test_materials)
predictions = model.predict(test_materials_scaled)
probabilities = model.predict_proba(test_materials_scaled)

print(f"\nHasil prediksi untuk material baru:\n")
for i, (name, pred, prob) in enumerate(zip(test_names, predictions, probabilities)):
    material_type = "KONDUKTOR" if pred == 1 else "ISOLATOR"
    confidence = prob[pred] * 100
    print(f"{i+1}. {name}")
    print(f"   Sifat fisis: log₁₀(ρ) = {test_materials[i,0]:.2f}, κ = {test_materials[i,1]:.2f} W/(m·K)")
    print(f"   → Prediksi: {material_type} (confidence: {confidence:.1f}%)")
    print(f"   → Probabilitas: Isolator={prob[0]*100:.1f}%, Konduktor={prob[1]*100:.1f}%")
    print()

# ============================================================================
# 10. KESIMPULAN DAN REKOMENDASI
# ============================================================================

print("="*70)
print("KESIMPULAN DAN REKOMENDASI")
print("="*70)
print()
print("HASIL UTAMA:")
print(f"  ✓ Model ANN berhasil mengklasifikasi material dengan akurasi {test_acc*100:.2f}%")
print(f"  ✓ Cross-validation menunjukkan konsistensi: {cv_scores.mean()*100:.2f}% (±{cv_scores.std()*100:.2f}%)")
print(f"  ✓ Fitur resistivitas lebih dominan dalam klasifikasi")
print()
print("REKOMENDASI PENGEMBANGAN:")
print("  • Tambahkan data material untuk meningkatkan generalisasi")
print("  • Eksplorasi ensemble methods untuk performa lebih baik")
print("  • Implementasi feature engineering untuk fitur tambahan")
print("  • Gunakan teknik regularization untuk mencegah overfitting")
print()
print("="*70)
print("PROGRAM SELESAI")
print("="*70)